{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2hdkkmTeisP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYzvG9rqepmo"
      },
      "outputs": [],
      "source": [
        "from skimage import data, color, feature\n",
        "import skimage.data\n",
        "\n",
        "image = color.rgb2gray(data.chelsea())\n",
        "hog_vec, hog_vis = feature.hog(image, visualize=True)\n",
        "print(len(hog_vec))\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "ax[0].imshow(image, cmap='gray')\n",
        "ax[0].set_title('input image')\n",
        "\n",
        "ax[1].imshow(hog_vis)\n",
        "ax[1].set_title('visualization of HOG features');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4PjpILqchqU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import cv2\n",
        "drive.mount('/content/drive')\n",
        "dataset_folder = \"/content/drive/MyDrive/haascades/\"\n",
        "face_cascade = cv2.CascadeClassifier(dataset_folder + 'haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDEjre2Xeprl"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Load the LFW dataset\n",
        "faces1 = fetch_lfw_people()\n",
        "\n",
        "# Load the Olivetti Faces dataset\n",
        "faces2 = fetch_olivetti_faces()\n",
        "\n",
        "# Extract images from both datasets\n",
        "patches1 = faces1.images\n",
        "patches2 = faces2.images\n",
        "\n",
        "desired_shape = (45, 45)\n",
        "positive_patches1_resized = [resize(image, desired_shape) for image in patches1]\n",
        "positive_patches2_resized = [resize(image, desired_shape) for image in patches2]\n",
        "\n",
        "# Concatenate the resized images from both datasets along the first axis (axis=0)\n",
        "positive_patches = np.concatenate([positive_patches1_resized, positive_patches2_resized], axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F4D_Jfd-Zzk"
      },
      "outputs": [],
      "source": [
        "# method 1\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "from skimage import data, color, feature\n",
        "import skimage.data\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_folder = \"/content/drive/MyDrive/haascades/\"\n",
        "input_directory = '/content/drive/MyDrive/extracted_images/'\n",
        "output_directory = '/content/drive/MyDrive/processed_images1/'\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(dataset_folder + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "def detect(img):\n",
        "  faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5, minSize=(45, 45))\n",
        "  try:\n",
        "    x, y, w, h = faces[0]\n",
        "    img = img[y:y+h, x:x+w]\n",
        "    img = Image.fromarray(img)\n",
        "    img = img.resize((45, 45))\n",
        "  except:\n",
        "    img = None\n",
        "  return img\n",
        "\n",
        "_imgfile = []\n",
        "\n",
        "def batch_process_images(input_directory, output_directory):\n",
        "    for filename in os.listdir(input_directory):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            input_path = os.path.join(input_directory, filename)\n",
        "            output_path = os.path.join(output_directory, filename)\n",
        "            try:\n",
        "                with Image.open(input_path) as img:\n",
        "                    img = color.rgb2gray(img)\n",
        "                    img = (img * 255).astype(np.uint8)\n",
        "                    img = detect(img)\n",
        "                    _imgfile.append(img)\n",
        "                    img.save(output_path)\n",
        "                    try:\n",
        "                      print(\"Thanh cong_Successfully: \"% filename)\n",
        "                    except AttributeError:\n",
        "                      print('\\n')\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "batch_process_images(input_directory, output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cÃ¡ch 2\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "from skimage import data, color, feature\n",
        "import skimage.data\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_imgidx = \"/content/drive/MyDrive/zip/CASIA-WebFace_crop.zip\"\n",
        "imgzip = zipfile.ZipFile(path_imgidx)\n",
        "inflist = imgzip.infolist()\n",
        "\n",
        "image_file = []\n",
        "cropped_img = []\n",
        "crop = []\n",
        "X_train = []\n",
        "npoint = 8\n",
        "rad = 1.0\n",
        "\n",
        "def detect(img, idx):\n",
        "  faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5, minSize=(45, 45))\n",
        "  try:\n",
        "    x, y, w, h = faces[0]\n",
        "    img = img[y:y+h, x:x+w]\n",
        "    img = cv2.resize(img, (45,45))\n",
        "  except:\n",
        "    img = None\n",
        "  return img\n",
        "\n",
        "\n",
        "for f in inflist:\n",
        "    ifile = imgzip.open(f)\n",
        "    img = Image.open(ifile)\n",
        "    img = color.rgb2gray(img)\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    image_file.append(img)\n",
        "\n",
        "for i, img in enumerate(image_file):\n",
        "    img = detect(img, i)\n",
        "    if img is not None:\n",
        "      img = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
        "      cropped_img.append(img)"
      ],
      "metadata": {
        "id": "NDXRvWmjcHeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_arr = []\n",
        "for i in range(len(processed_image)):\n",
        "  arr = np.array(processed_image[i])\n",
        "  imgs = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)\n",
        "  n_arr.append(imgs)\n",
        "\n",
        "plt.imshow(n_arr[0])"
      ],
      "metadata": {
        "id": "leHvwZUxsOJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhYWQTXD5gXR"
      },
      "outputs": [],
      "source": [
        "positive_patches = np.concatenate([positive_patches, cropped_img],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEgpBrXmhHAO"
      },
      "outputs": [],
      "source": [
        "from skimage import data, transform\n",
        "\n",
        "imgs_to_use = ['camera', 'text', 'coins', 'moon',\n",
        "               'page', 'clock', 'immunohistochemistry',\n",
        "               'chelsea', 'coffee', 'hubble_deep_field']\n",
        "images = [getattr(data, name)()\n",
        "          for name in imgs_to_use]\n",
        "for i, data in enumerate(images):\n",
        "  if len(data.shape) > 2:\n",
        "    images[i] = color.rgb2gray(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVj2ispqhl0K"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.image import PatchExtractor\n",
        "\n",
        "def extract_patches(img, N, scale=1.0, patch_size=positive_patches[0].shape):\n",
        "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
        "    extractor = PatchExtractor(patch_size=extracted_patch_size,\n",
        "                               max_patches=N, random_state=0)\n",
        "    patches = extractor.transform(img[np.newaxis])\n",
        "    if scale != 1:\n",
        "        patches = np.array([transform.resize(patch, patch_size)\n",
        "                            for patch in patches])\n",
        "    return patches\n",
        "\n",
        "negative_patches = np.vstack([extract_patches(im, 1000, scale)\n",
        "                              for im in images for scale in [0.5, 1.0, 2.0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-uT95CDz7Fi"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "path_imgidx = \"/content/drive/MyDrive/zip/CASIA-WebFace_crop.zip\"\n",
        "imgzip = zipfile.ZipFile(path_imgidx)\n",
        "inflist = imgzip.infolist()\n",
        "\n",
        "image_file = []\n",
        "cropped_img = []\n",
        "\n",
        "def detect(img, idx):\n",
        "  faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5, minSize=(45, 45))\n",
        "  try:\n",
        "    x, y, w, h = faces[0]\n",
        "    img = img[y:y+h, x:x+w]\n",
        "    img = cv2.resize(img, (45,45))\n",
        "  except:\n",
        "    img = None\n",
        "  return img\n",
        "\n",
        "for f in inflist:\n",
        "    ifile = imgzip.open(f)\n",
        "    img = Image.open(ifile)\n",
        "    img = color.rgb2gray(img)\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    image_file.append(img)\n",
        "\n",
        "for i, img in enumerate(image_file):\n",
        "    img = detect(img, i)\n",
        "    if img is not None:\n",
        "      cropped_img.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRpPxaVAgcmb"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "_train = []\n",
        "\n",
        "npoint = 8\n",
        "rad = 1.0\n",
        "for im in chain(positive_patches, negative_patches):\n",
        "    # Compute LBP features\n",
        "    lbp_feature = feature.local_binary_pattern(im, npoint, rad)\n",
        "    lbp_feature = np.histogram(lbp_feature.ravel(), bins=np.arange(0, npoint + 3), range=(0, npoint + 2))\n",
        "    lbp_feature = lbp_feature[0]\n",
        "\n",
        "    # Compute HOG features\n",
        "    hog_feature = feature.hog(im)\n",
        "\n",
        "    # Concatenate LBP and HOG features\n",
        "    concatenated_feature = np.concatenate((hog_feature, lbp_feature))\n",
        "    _train.append(concatenated_feature)\n",
        "\n",
        "X = np.array(_train)\n",
        "y = np.zeros(X.shape[0])\n",
        "y[:positive_patches.shape[0]] = 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, test_size=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_kD8_0egBT8"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(GaussianNB(), X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTvMkJTokvGZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid = GridSearchCV(LinearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]})\n",
        "grid.fit(X_train, y_train)\n",
        "grid.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-2U7BA8lkn6"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import joblib\n",
        "\n",
        "model = make_pipeline(StandardScaler(), grid.best_estimator_)\n",
        "# model = make_pipeline(StandardScaler(), SVC(C=1, gamma='auto', probability=True))\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "id": "ZcCsVss7ndV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model, \"best_model.pkl\")"
      ],
      "metadata": {
        "id": "5dfUebEBi15H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl5RwGy3fX9r"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"sample_data/1.jpg\")\n",
        "test_image = img\n",
        "test_image = skimage.color.rgb2gray(test_image)\n",
        "test_image = skimage.transform.rescale(test_image, 0.5)\n",
        "\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoAcS1uBfbbk"
      },
      "outputs": [],
      "source": [
        "def sliding_window(img, patch_size=positive_patches[0].shape,\n",
        "                   istep=2, jstep=2, scale=1.0):\n",
        "    Ni, Nj = (int(scale * s) for s in patch_size)\n",
        "    for i in range(0, img.shape[0] - Ni, istep):\n",
        "        for j in range(0, img.shape[1] - Ni, jstep):\n",
        "            patch = img[i:i + Ni, j:j + Nj]\n",
        "            if scale != 1:\n",
        "                patch = transform.resize(patch, patch_size)\n",
        "            yield (i, j), patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBvTwNJ9f1fS"
      },
      "outputs": [],
      "source": [
        "indices, patches = zip(*sliding_window(test_image))\n",
        "# patches_hog = np.array([feature.hog(patch) for patch in patches])\n",
        "\n",
        "\n",
        "npoint = 8\n",
        "rad = 1.0\n",
        "patches_hog = []\n",
        "for im in patches:\n",
        "    print(im.shape)\n",
        "    # Compute LBP features\n",
        "    lbp_feature = feature.local_binary_pattern(im, npoint, rad)\n",
        "    lbp_feature = np.histogram(lbp_feature.ravel(), bins=np.arange(0, npoint + 3), range=(0, npoint + 2))\n",
        "    lbp_feature = lbp_feature[0]\n",
        "\n",
        "    # Compute HOG features\n",
        "    hog_feature = feature.hog(im)\n",
        "\n",
        "    # Concatenate LBP and HOG features\n",
        "\n",
        "    concatenated_feature = np.concatenate((hog_feature, lbp_feature))\n",
        "    result = model.predict_proba([concatenated_feature])\n",
        "\n",
        "    # Append to the list of feature vectors\n",
        "    patches_hog.append(concatenated_feature)\n",
        "\n",
        "patches_hog = np.array(patches_hog)\n",
        "\n",
        "\n",
        "# patches_hog.shape\n",
        "confidence_scores = model.decision_function(patches_hog)\n",
        "labels = model.predict(patches_hog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF4vgn8Ml20r"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(test_image, cmap='gray')\n",
        "ax.axis('off')\n",
        "\n",
        "Ni, Nj = positive_patches[0].shape\n",
        "\n",
        "indices = np.array(indices)\n",
        "\n",
        "for i, j in indices[labels == 1]:\n",
        "    ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',\n",
        "                               alpha=0.3, lw=2, facecolor='none'))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
